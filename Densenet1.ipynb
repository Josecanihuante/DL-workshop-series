{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josecanihuante/DL-workshop-series/blob/master/Densenet1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6aDV7y3u3pq"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "import keras.backend as K\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6OAsw8rvH5K"
      },
      "outputs": [],
      "source": [
        "n_class = 5\n",
        "\n",
        "# path to kaggle dataset\n",
        "root_path = \"/content/drive/MyDrive/ArtrosisdeRodilla/KneeXrayData/KneeXrayData/ClsKLData/kneeKL224/\"\n",
        "\n",
        "# list of folders\n",
        "folder_list = os.listdir(root_path)\n",
        "image_path_list = []\n",
        "label_list = []\n",
        "\n",
        "# for each folder, get the image path and labels\n",
        "for folder in folder_list:\n",
        "    for label in range(n_class):\n",
        "        \n",
        "        # get all the images path inside the current folder\n",
        "        image_list = os.listdir(f\"{root_path}{folder}/{label}\")\n",
        "        # add to the image path list\n",
        "        image_path_list += [ f\"{root_path}{folder}/{label}/\"+ path for path in image_list]\n",
        "        \n",
        "        # add labels to the label list\n",
        "        label_list += [label] * len(image_list)\n",
        "\n",
        "# convert to dataframe\n",
        "df_train_kaggle = pd.DataFrame({\"filepath\" : image_path_list, \"label\": label_list})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SAw0QrUvuyC"
      },
      "outputs": [],
      "source": [
        "# train data generator object\n",
        "train_aug = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# validation data generator object\n",
        "valid_aug = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGii1i7Xvvzb",
        "outputId": "5fa69941-463c-4416-b049-901fbf0592bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9867 validated image filenames.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_aug.flow_from_dataframe(\n",
        "dataframe=df_train_kaggle,\n",
        "directory=None,\n",
        "x_col=\"filepath\",\n",
        "y_col=\"label\",\n",
        "batch_size=32,\n",
        "seed=42,\n",
        "shuffle=True,\n",
        "class_mode=\"raw\",\n",
        "target_size=(224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fAqubdj0WaL"
      },
      "outputs": [],
      "source": [
        "!unzip -q -o /content/KneeXray.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb5ou6sE0Y75"
      },
      "outputs": [],
      "source": [
        "compi_root_path= \"/content/KneeXray/\"\n",
        "df_val_compi = pd.read_csv(compi_root_path + \"Train.csv\")\n",
        "\n",
        "# add absolute path to the image names\n",
        "df_val_compi[\"filename\"] = df_val_compi.filename.apply(lambda x: compi_root_path+\"train/\" + x)\n",
        "df_val_compi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2W3yh800dtR",
        "outputId": "3132874c-a9f5-453c-8c04-7cd08065cd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7828 validated image filenames.\n"
          ]
        }
      ],
      "source": [
        "# create validation generator\n",
        "valid_generator = valid_aug.flow_from_dataframe( \n",
        "dataframe= df_val_compi,\n",
        "x_col= \"filename\",\n",
        "y_col= \"label\",\n",
        "batch_size= 32,\n",
        "seed= 42,\n",
        "shuffle= True,\n",
        "class_mode= \"raw\",\n",
        "target_size= (224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSTjOKee0euh"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as K\n",
        "\n",
        "initializer = K.initializers.he_normal(seed=32)\n",
        "input_shape_densenet = (224, 224, 3)\n",
        "\n",
        "densenet_model = K.applications.DenseNet169(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=input_shape_densenet,\n",
        "    pooling=None\n",
        ")\n",
        "\n",
        "\n",
        "densenet_model.trainable = True\n",
        "\n",
        "for layer in densenet_model.layers:\n",
        "  if 'conv5' in layer.name:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "\n",
        "densenet_model.summary()\n",
        "\n",
        "input = K.Input(shape=(224, 224, 3))\n",
        "\n",
        "layer = densenet_model(inputs=input)\n",
        "\n",
        "layer = K.layers.Flatten()(layer)\n",
        "\n",
        "layer = K.layers.BatchNormalization()(layer)\n",
        "\n",
        "layer = K.layers.Dense(units=256,\n",
        "                        activation='relu',\n",
        "                        kernel_initializer=initializer\n",
        "                        )(layer)\n",
        "\n",
        "layer = K.layers.Dropout(0.4)(layer)\n",
        "\n",
        "layer = K.layers.BatchNormalization()(layer)\n",
        "\n",
        "layer = K.layers.Dense(units=128,\n",
        "                       activation='relu',\n",
        "                       kernel_initializer=initializer\n",
        "                       )(layer)\n",
        "\n",
        "layer = K.layers.Dropout(0.4)(layer)\n",
        "\n",
        "layer = K.layers.Dense(units=5,\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer=initializer\n",
        "                       )(layer)\n",
        "\n",
        "model = K.models.Model(inputs=input, outputs=layer)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001,decay=0.0001),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/'\n",
        "\n",
        "my_callbacks = tf.keras.callbacks.ModelCheckpoint(\n",
        "    '/content',\n",
        "    monitor=\"val_loss\",\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode=\"auto\",\n",
        "    save_freq=\"epoch\",\n",
        "    options=None,\n",
        "    initial_value_threshold=None,\n",
        "    )"
      ],
      "metadata": {
        "id": "styJJH7mbYQt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 classes= np.unique(df_train_kaggle.label.values),\n",
        "                                                 y= df_train_kaggle.label.values)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "hznCJXBNYKQi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a8KRg7ZB44Qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21559994-07be-46ea-dcdd-a83483720084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 2.1524 - accuracy: 0.2988 INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 7770s 25s/step - loss: 2.1524 - accuracy: 0.2988 - val_loss: 1.1341 - val_accuracy: 0.5584\n",
            "Epoch 2/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 1.6560 - accuracy: 0.4273INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 168s 544ms/step - loss: 1.6560 - accuracy: 0.4273 - val_loss: 0.8652 - val_accuracy: 0.6712\n",
            "Epoch 3/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 1.4119 - accuracy: 0.4913INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 168s 545ms/step - loss: 1.4119 - accuracy: 0.4913 - val_loss: 0.6942 - val_accuracy: 0.7487\n",
            "Epoch 4/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 1.2362 - accuracy: 0.5509INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 168s 544ms/step - loss: 1.2362 - accuracy: 0.5509 - val_loss: 0.5766 - val_accuracy: 0.8016\n",
            "Epoch 5/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 1.0933 - accuracy: 0.5912INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 168s 546ms/step - loss: 1.0933 - accuracy: 0.5912 - val_loss: 0.4843 - val_accuracy: 0.8522\n",
            "Epoch 6/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 0.9653 - accuracy: 0.6383INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 168s 545ms/step - loss: 0.9653 - accuracy: 0.6383 - val_loss: 0.4023 - val_accuracy: 0.8885\n",
            "Epoch 7/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 0.8576 - accuracy: 0.6748INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 168s 545ms/step - loss: 0.8576 - accuracy: 0.6748 - val_loss: 0.3427 - val_accuracy: 0.9080\n",
            "Epoch 8/8\n",
            "309/309 [==============================] - ETA: 0s - loss: 0.7853 - accuracy: 0.7011INFO:tensorflow:Assets written to: /content/assets\n",
            "309/309 [==============================] - 169s 547ms/step - loss: 0.7853 - accuracy: 0.7011 - val_loss: 0.2851 - val_accuracy: 0.9387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f83f43390d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "history = model.fit(train_generator, epochs=8, validation_data=(valid_generator), callbacks=[my_callbacks], batch_size=32, verbose=1, use_multiprocessing = True,\n",
        "        workers = -1)\n",
        "\n",
        "model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6i3-QR55SvrH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(df_val_compi,\n",
        "                                   test_size=0.1,\n",
        "                                   random_state=42,\n",
        "                                   stratify= df_val_compi.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w7vXpHZUboQ",
        "outputId": "325479d8-4d17-43bd-b311-6f4812d35691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7045 validated image filenames.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_aug.flow_from_dataframe(\n",
        "dataframe = X_train,\n",
        "x_col=\"filename\",\n",
        "y_col=\"label\",\n",
        "batch_size=32,\n",
        "seed=42,\n",
        "shuffle=True,\n",
        "class_mode=\"raw\",\n",
        "target_size=(224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1h5iUZIUeZh",
        "outputId": "227ed300-3d7c-4df8-a4d5-9cd5b58fe618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 783 validated image filenames.\n"
          ]
        }
      ],
      "source": [
        "valid_generator = valid_aug.flow_from_dataframe( \n",
        "dataframe=X_test,\n",
        "x_col=\"filename\",\n",
        "y_col=\"label\",\n",
        "batch_size=32,\n",
        "seed=42,\n",
        "shuffle=True,\n",
        "class_mode=\"raw\",\n",
        "target_size=(224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Kc_3qHJjUgnC"
      },
      "outputs": [],
      "source": [
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID =valid_generator.n//valid_generator.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "233to3IwUhqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1559a4aa-72ab-47c2-ba96-71067bc399de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "220/220 [==============================] - 39s 179ms/step - loss: 0.7459 - accuracy: 0.7170 - val_loss: 0.2890 - val_accuracy: 0.9284\n",
            "Epoch 2/10\n",
            "220/220 [==============================] - 38s 171ms/step - loss: 0.6763 - accuracy: 0.7425 - val_loss: 0.3010 - val_accuracy: 0.9180\n",
            "Epoch 3/10\n",
            "220/220 [==============================] - 37s 170ms/step - loss: 0.6027 - accuracy: 0.7764 - val_loss: 0.3276 - val_accuracy: 0.9141\n",
            "Epoch 4/10\n",
            "220/220 [==============================] - 38s 170ms/step - loss: 0.5578 - accuracy: 0.7948 - val_loss: 0.3334 - val_accuracy: 0.9102\n",
            "Epoch 5/10\n",
            "220/220 [==============================] - 37s 170ms/step - loss: 0.5165 - accuracy: 0.8084 - val_loss: 0.3340 - val_accuracy: 0.9089\n",
            "Epoch 6/10\n",
            "220/220 [==============================] - 38s 171ms/step - loss: 0.4534 - accuracy: 0.8326 - val_loss: 0.3305 - val_accuracy: 0.9049\n",
            "Epoch 7/10\n",
            "220/220 [==============================] - 38s 171ms/step - loss: 0.4295 - accuracy: 0.8451 - val_loss: 0.3466 - val_accuracy: 0.8893\n",
            "Epoch 8/10\n",
            "220/220 [==============================] - 38s 170ms/step - loss: 0.3804 - accuracy: 0.8630 - val_loss: 0.3488 - val_accuracy: 0.8958\n",
            "Epoch 9/10\n",
            "220/220 [==============================] - 38s 171ms/step - loss: 0.3534 - accuracy: 0.8708 - val_loss: 0.3507 - val_accuracy: 0.8828\n",
            "Epoch 10/10\n",
            "220/220 [==============================] - 38s 171ms/step - loss: 0.3240 - accuracy: 0.8878 - val_loss: 0.3565 - val_accuracy: 0.8711\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83f3725210>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "        epochs=10,\n",
        "        validation_data=valid_generator,\n",
        "        validation_steps=STEP_SIZE_VALID, callbacks=[my_callbacks])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "sm6BLf4ebV0G",
        "outputId": "7babbe3f-4930-4142-dc78-6bbc593d41ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f83f0e38a50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "target_shape = 224\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# test generator\n",
        "compi_gen = valid_aug.flow_from_dataframe(dataframe= X_test,\n",
        "                            x_col= \"filename\",\n",
        "                            class_mode=None,\n",
        "                            target_size= (target_shape, target_shape),\n",
        "                            shuffle= False,\n",
        "                            batch_size= BATCH_SIZE\n",
        "                            )"
      ],
      "metadata": {
        "id": "4cmlY2JCzr8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6a5749-327f-476b-a225-014d76ed7fdd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 783 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicition_compi = model.predict(compi_gen, steps= compi_gen.n/ BATCH_SIZE, verbose= 1)"
      ],
      "metadata": {
        "id": "1immC_Ilzt9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8d3a7c-6e6a-4e03-d405-f2767002e038"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "783/783 [==============================] - 20s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "class_prediction_compi =  np.argmax(predicition_compi, axis= 1)\n",
        "cm = confusion_matrix(X_test.label, class_prediction_compi, labels=[0, 1, 2, 3, 4])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[0, 1, 2, 3, 4])\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "MbwtjsDkzwCH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "c713964d-1c0e-4c87-9c4e-acda8f0a9364"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f84d83d4350>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_path)\n"
      ],
      "metadata": {
        "id": "ipQtO5wnqp26",
        "outputId": "b6cb30b2-7b47-427c-f552-3183d0d3e51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f49f27f9850>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "VNhkv4uG2nrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_test = valid_aug.flow_from_directory('/content/KneeXray/Test.csv.iloc[384]',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical') "
      ],
      "metadata": {
        "id": "uC77Ijvsivzx",
        "outputId": "325aded5-8775-47d6-9cae-6ea1f995517f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-47414bc4f68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                             \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                             class_mode = 'categorical') \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         )\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/KneeXray/Test.csv.iloc[384]'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.iloc[23]"
      ],
      "metadata": {
        "id": "c_85cuap0cgF",
        "outputId": "d83b14b3-5417-43fd-b06f-9570e35ce740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "filename    /content/KneeXray/train/Image_4157.jpg\n",
              "label                                            0\n",
              "Name: 4156, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Densenet1.ipynb",
      "provenance": [],
      "mount_file_id": "1Yg_pF5rUnd3k8Vzv08XU2VKVl-sQUGSg",
      "authorship_tag": "ABX9TyO7JrdsBw/Gd2lel9IUarzu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}